@article{Wasserman2000,
abstract = {This paper reviews the Bayesian approach to model selection and model averaging. In this review, I emphasize objective Bayesian methods based on noninformative priors. I will also discuss implementation details, approximations, and relationships to other methods.},
author = {Wasserman, Larry},
doi = {10.1006/JMPS.1999.1278},
issn = {0022-2496},
journal = {Journal of Mathematical Psychology},
month = {mar},
number = {1},
pages = {92--107},
publisher = {Academic Press},
title = {{Bayesian Model Selection and Model Averaging}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S0022249699912786},
volume = {44},
year = {2000}
}
@inproceedings{VanderPlas2014,
abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
archivePrefix = {arXiv},
arxivId = {1411.5018},
author = {VanderPlas, Jake},
booktitle = {Proceedings of the 13th Python in Science Conference},
doi = {10.25080/majora-14bd3278-00e},
eprint = {1411.5018},
title = {{Frequentism and Bayesianism: A Python-driven Primer}},
year = {2014}
}
@article{Williams1989,
abstract = {In the face of the ever-increasing importance of statistical methods in medical research and practice, the first edition of this publication has provided a sound and deep understanding of statistical methods in bioassay to many students and researchers. In addition to the profound presentation of statistical methods of the first edition, here the reader will find new material stemming from the recent statistical literature as well as data reflecting modern trends in general applied statistical research. Examples are discussions on design and planning, e.g. choices of dose levels, and additional section in the chapter on Bayes methods, and a new chapter on sequential estimation for the logistic model. The book will be a valuable source of information to students in the experimental area of statistical aspects of biological assay, professional statisticians with an interest in research in this topic, teachers in statistics and biology, and investigators in the biological and medical sciences who use bioassay in their work.},
author = {Williams, D. A. and Govindarjulu, Z.},
doi = {10.2307/2531710},
issn = {0006341X},
journal = {Biometrics},
title = {{Statistical Techniques in Bioassay.}},
year = {1989}
}
@article{Gottschalk2005,
abstract = {Improvements in assay technology have reduced the amount of random variation in measured responses to the point where even slight asymmetry of the assay data can be more significant than random variation. Use of the five-parameter logistic (5PL) function to fit dose-response data easily accommodates such asymmetry. The 5PL can dramatically improve the accuracy of asymmetric assays over the use of symmetric models such as the four-parameter logistic (4PL) function. Until recently, however, the process of fitting the 5PL function has been difficult, with the result that the 4PL function has continued to be used even for highly asymmetric data. Various ad hoc modifications of the 4PL method have been developed in an attempt to address asymmetric data. However, recent advances in numerical methods and assay analysis software have rendered easier the fitting of the 5PL routine. This paper demonstrates how use of the 5PL function can improve assay performance over the 4PL and its variants. Specifically, the improvement in the accuracy of concentration estimates that can be obtained using the 5PL over the 4PL as a function of the asymmetry present in the data is studied. The behavior of the 5PL curve and how it differs from the 4PL curve are discussed. Common experimental designs, which can lead to ill-conditioned regression problems, are also examined. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Gottschalk, Paul G. and Dunn, John R.},
doi = {10.1016/j.ab.2005.04.035},
issn = {00032697},
journal = {Analytical Biochemistry},
keywords = {4PL,5PL,Bioassay,Curve model,Data reduction,Dose-response curve,Immunoassay,Statistical analysis},
title = {{The five-parameter logistic: A characterization and comparison with the four-parameter logistic}},
year = {2005}
}
@misc{Sala-i-Martin2004,
abstract = {This paper examines the robustness of explanatory variables in cross-country economic growth regressions. It introduces and employs a novel approach, Bayesian Averaging of Classical Estimates (BACE), which constructs estimates by averaging OLS coefficients across models. The weights given to individual regressions have a Bayesian justification similar to the Schwarz model selection criterion. Of 67 explanatory variables we find 18 to be significantly and robustly partially correlated with long-term growth and another three variables to be marginally related. The strongest evidence is for the relative price of investment, primary school enrollment, and the initial level of real GDP per capita.},
author = {Sala-i-Martin, Xavier and Doppelhofer, Gernot and Miller, Ronald I.},
booktitle = {American Economic Review},
doi = {10.1257/0002828042002570},
issn = {00028282},
title = {{Determinants of long-term growth: A bayesian averaging of classical estimates (BACE) approach}},
year = {2004}
}
@book{Claeskens2008,
abstract = {Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled with discussions of frequent and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R-code.},
author = {Claeskens, Gerda and Hjort, Nils Lid},
booktitle = {Model Selection and Model Averaging},
doi = {10.1017/CBO9780511790485},
isbn = {9780511790485},
title = {{Model selection and model averaging}},
year = {2008}
}
@article{Nguimkeu2014,
abstract = {This paper proposes a simple model selection test between the Gompertz and the Logistic growth models based on parameter significance testing in a comprehensive linear regression. Simulations studies are provided to show the accuracy of the method. Two real-data examples are also provided to illustrate the implementation of the proposed method in practice. {\textcopyright} 2014 Elsevier Inc.},
author = {Nguimkeu, Pierre},
doi = {10.1016/j.techfore.2014.06.017},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
keywords = {Gompertz function,Logistic function,Model selection,T-test},
title = {{A simple selection test between the Gompertz and Logistic growth models}},
year = {2014}
}
@incollection{Kery2016,
abstract = {Class I myosins participate in various interactions between the cell membrane and the cytoskeleton. Several class I myosins preferentially bind to acidic phospholipids, such as phosphatidylserine and phosphatidylinositol 4,5-bisphosphate [PI(4,5)P(2)], through a tail homology 1 (TH1) domain. Here, we show that the second messenger lipid phosphatidylinositol 3,4,5-trisphosphate (PIP(3)) binds to the TH1 domain of a subset of Dictyostelium class I myosins (ID, IE, and IF) and recruits them to the plasma membrane. The PIP(3)-regulated membrane recruitment of myosin I promoted chemotaxis and induced chemoattractant-stimulated actin polymerization. Similarly, PIP(3) recruited human myosin IF to the plasma membrane upon chemotactic stimulation in a neutrophil cell line. These data suggest a mechanism through which the PIP(3) signal is transmitted through myosin I to the actin cytoskeleton},
author = {K{\'{e}}ry, Marc and Royle, J. Andrew},
booktitle = {Applied Hierarchical Modeling in Ecology},
doi = {10.1016/b978-0-12-801378-6.00004-7},
title = {{Introduction to Data Simulation}},
year = {2016}
}
@book{Brooks2011,
abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory. The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology. The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
author = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao Li},
booktitle = {Handbook of Markov Chain Monte Carlo},
isbn = {9781420079425},
title = {{Handbook of Markov Chain Monte Carlo}},
year = {2011}
}
@article{Murari2019,
abstract = {The most widely used forms of model selection criteria, the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC), are expressed in terms of synthetic indicators of the residual distribution: the variance and the mean-squared error of the residuals respectively. In many applications in science, the noise affecting the data can be expected to have a Gaussian distribution. Therefore, at the same level of variance and mean-squared error, models, whose residuals are more uniformly distributed, should be favoured. The degree of uniformity of the residuals can be quantified by the Shannon entropy. Including the Shannon entropy in the BIC and AIC expressions improves significantly these criteria. The better performances have been demonstrated empirically with a series of simulations for various classes of functions and for different levels and statistics of the noise. In presence of outliers, a better treatment of the errors, using the Geodesic Distance, has proved essential.},
author = {Murari, Andrea and Peluso, Emmanuele and Cianfrani, Francesco and Gaudio, Pasquale and Lungaroni, Michele},
doi = {10.3390/e21040394},
issn = {10994300},
journal = {Entropy},
keywords = {Akaike Information Criterion (AIC),Bayesian Information Criterion (BIC),Geodesic Distance,Model Selection Criteria,Shannon Entropy},
title = {{On the use of entropy to improve model selection criteria}},
year = {2019}
}
@article{VanDerPloeg2014,
abstract = {Background: Modern modelling techniques may potentially provide more accurate predictions of binary outcomes than classical techniques. We aimed to study the predictive performance of different modelling techniques in relation to the effective sample size ("data hungriness"). Methods: We performed simulation studies based on three clinical cohorts: 1282 patients with head and neck cancer (with 46.9{\%} 5 year survival), 1731 patients with traumatic brain injury (22.3{\%} 6 month mortality) and 3181 patients with minor head injury (7.6{\%} with CT scan abnormalities). We compared three relatively modern modelling techniques: support vector machines (SVM), neural nets (NN), and random forests (RF) and two classical techniques: logistic regression (LR) and classification and regression trees (CART). We created three large artificial databases with 20 fold, 10 fold and 6 fold replication of subjects, where we generated dichotomous outcomes according to different underlying models. We applied each modelling technique to increasingly larger development parts (100 repetitions). The area under the ROC-curve (AUC) indicated the performance of each model in the development part and in an independent validation part. Data hungriness was defined by plateauing of AUC and small optimism (difference between the mean apparent AUC and the mean validated AUC {\textless}0.01). Results: We found that a stable AUC was reached by LR at approximately 20 to 50 events per variable, followed by CART, SVM, NN and RF models. Optimism decreased with increasing sample sizes and the same ranking of techniques. The RF, SVM and NN models showed instability and a high optimism even with {\textgreater}200 events per variable. Conclusions: Modern modelling techniques such as SVM, NN and RF may need over 10 times as many events per variable to achieve a stable AUC and a small optimism than classical modelling techniques such as LR. This implies that such modern techniques should only be used in medical prediction problems if very large data sets are available.},
author = {{Van Der Ploeg}, Tjeerd and Austin, Peter C. and Steyerberg, Ewout W.},
doi = {10.1186/1471-2288-14-137},
issn = {14712288},
journal = {BMC Medical Research Methodology},
title = {{Modern modelling techniques are data hungry: A simulation study for predicting dichotomous endpoints}},
year = {2014}
}
@article{Harris2016,
abstract = {The growth rate and carrying capacity of a cell population are key to the characterization of the population's viability and to the quantification of its responses to perturbations such as drug treatments. Accurate estimation of these parameters necessitates careful analysis. Here, we present a rigorous mathematical approach for the robust analysis of cell count data, in which all the experimental stages of the cell counting process are investigated in detail with the machinery of Bayesian probability theory. We advance a flexible theoretical framework that permits accurate estimates of the growth parameters of cell populations and of the logical correlations between them. Moreover, our approach naturally produces an objective metric of avoidable experimental error, which may be tracked over time in a laboratory to detect instrumentation failures or lapses in protocol. We apply our method to the analysis of cell count data in the context of a logistic growth model by means of a user-friendly computer program that automates this analysis, and present some samples of its output. Finally, we note that a traditional least squares fit can provide misleading estimates of parameter values, because it ignores available information with regard to the way in which the data have actually been collected.},
author = {Harris, Edouard A. and Koh, Eun Jee and Moffat, Jason and McMillen, David R.},
doi = {10.1103/PhysRevE.93.012402},
issn = {24700053},
journal = {Physical Review E},
title = {{Automated inference procedure for the determination of cell growth parameters}},
year = {2016}
}
@article{Amaran2016,
abstract = {Simulation optimization (SO) refers to the optimization of an objective function subject to constraints, both of which can be evaluated through a stochastic simulation. To address specific features of a particular simulation—discrete or continuous decisions, expensive or cheap simulations, single or multiple outputs, homogeneous or heterogeneous noise—various algorithms have been proposed in the literature. As one can imagine, there exist several competing algorithms for each of these classes of problems. This document emphasizes the difficulties in SO as compared to algebraic model-based mathematical programming, makes reference to state-of-the-art algorithms in the field, examines and contrasts the different approaches used, reviews some of the diverse applications that have been tackled by these methods, and speculates on future directions in the field.},
archivePrefix = {arXiv},
arxivId = {1706.08591},
author = {Amaran, Satyajith and Sahinidis, Nikolaos V. and Sharda, Bikram and Bury, Scott J.},
doi = {10.1007/s10479-015-2019-x},
eprint = {1706.08591},
issn = {15729338},
journal = {Annals of Operations Research},
keywords = {Derivative-free optimization,Optimization via simulation,Simulation optimization},
title = {{Simulation optimization: a review of algorithms and applications}},
year = {2016}
}
@article{Montagna2017,
abstract = {The impact of mobile technologies on healthcare is particularly evident in the case of self-management of chronic diseases, where they can decrease spending and improve life quality of patients. We propose the adoption of agent-based modeling and simulation techniques as built-in tools to dynamically monitor the state of patient health and provide recommendations for self-management. To demonstrate the feasibility of our proposal we focus on Type 1 diabetes mellitus as our case study, and provide simulation results where the dynamic evolution of signal parameters is shown in the case of healthy and Type 1 diabetes mellitus patients, focussing in particular on the beneficial effects that self-management interventions have on plasma glucose values.},
author = {Montagna, Sara and Omicini, Andrea},
doi = {10.1177/0037549717712605},
issn = {17413133},
journal = {Simulation},
keywords = {Agent-based modeling,healthcare,self-management},
title = {{Agent-based modeling for the self-management of chronic diseases: An exploratory study}},
year = {2017}
}
@article{Fu2000,
abstract = {The integration of optimization and simulation has become nearly ubiquitous in practice as most discrete-event simulation packages now include some type of optimization routine. This panel session's objective was to explore the present state of the art in simulation optimization, prevailing issues for researchers, and future prospects for the field. The composition of the panel included views from both simulation software developers and academic researchers. This Proceedings paper begins with a brief overview of some issues, introduced by the chairman and organizer of the session, followed by the position statements of the panel members, which served as a starting point for the panel discussion.},
author = {Fu, Michael C. and Andrad{\'{o}}ttir, Sigr{\'{u}}n and Carson, John S. and Glover, Fred and Harrell, Charles R. and Ho, Yu Chi and Kelly, James P. and Robinson, Stephen M.},
doi = {10.1109/WSC.2000.899770},
issn = {02750708},
journal = {Winter Simulation Conference Proceedings},
title = {{Integrating optimization and simulation: Research and practice}},
year = {2000}
}
@article{Snider2013,
author = {{Snider and Brimlow}, J. N.},
journal = {Nature Education Knowledge},
number = {4},
pages = {3},
title = {{An Introduction to Population Growth.}},
volume = {4},
year = {2013}
}
@article{Buehler1991,
abstract = {We determined the relationship between bald eagle (Haliaeetusleucocephalus) distribution and human activity on the northern Chesapeake Bay shoreline during 1985-89. Only 55 of 1,117 locations of radio-tagged eagles (4.9{\%}) occurred in the developed land-cover type ({\~{}}4 buildings/4 ha), although 18.2{\%} of potential eagle habitat was developed (x' = 428.9, 4 df, P {\textless} 0.001). Eagle use of the shoreline was inversely related to building density (x' = 22.1, P {\textless} 0.001) and directly related to the development set-back distance (x' = 5.3, P = 0.02). Few eagles used shoreline segments with boats or pedestrians nearby (P {\textless} 0.001). Only 360 of 2,532 segments (14.2{\%}) had neither human activity nor shoreline development. Eagle flush distances because of approaching boats were greater in winter than in summer (f = 264.9 vs. 175.5 m, respectively, P = 0.001), but were similar for adult and immature eagles (f = 203.7 vs. 228.6 m, respectively, P = 0.38). Of 2,472 km of shoreline on the northern Chesapeake, 894 km (36.2{\%}) appears to be too developed to be suitable for eagle use, and an additional 996 km (40.3{\%}) had buildings within 500 m, thereby reducing eagle use. The projected increase in developed land in Maryland (74{\%}) and Virginia (80{\%}) from 1978 to 2020 is likely to determine the future of the bald eagle population on the northern Chesapeake Bay.},
author = {Buehler, David A. and Mersmann, Timothy J. and Fraser, James D. and Seegar, Janis K. D.},
doi = {10.2307/3809151},
issn = {0022541X},
journal = {The Journal of Wildlife Management},
title = {{Effects of Human Activity on Bald Eagle Distribution on the Northern Chesapeake Bay}},
year = {1991}
}
@article{Cho2010,
abstract = {We use Brownian dynamics simulations to analyze the electrophoretic separation of $\lambda$-DNA (48.5 kbp) and T4-DNA (169 kbp) in a hexagonal array of 1$\mu$m diameter posts with a 3$\mu$m center-to-center distance. The simulation method takes advantage of an efficient interpolation algorithm for the non-uniform electric field to reach an ensemble size (100 molecules) and simulation length scale (1 mm) that produces meaningful results for the average electrophoretic mobility and effective diffusion (dispersion) coefficient of these macromolecules as they move through the array. While the simulated electrophoretic mobility for $\lambda$-DNA is close to the experimental data, the simulation underestimates the magnitude of the corresponding dispersion coefficient. The simulations predict baseline resolution in a 15 mm device after 7 min using an electric field around 30 V/cm, with the resolution increasing exponentially as the electric field further decreases. The mobility and dispersivity data point out two essential phenomena that have been overlooked in previous models of DNA electrophoresis in post arrays: the relaxation time between collisions and simultaneous collisions with multiple posts. {\textcopyright} 2010 Elsevier B.V.},
author = {Cho, Jaeseol and Dorfman, Kevin D.},
doi = {10.1016/j.chroma.2010.06.057},
issn = {00219673},
journal = {Journal of Chromatography A},
keywords = {Computational modeling,DNA electrophoresis,Lab on a chip,Simulation},
title = {{Brownian dynamics simulations of electrophoretic DNA separations in a sparse ordered post array}},
year = {2010}
}
@article{Ding2018,
abstract = {In the era of big data, analysts usually explore various statistical models or machine-learning methods for observed data to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus it is central to scientific studies in such fields as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods has been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to provide a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-of-the-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
archivePrefix = {arXiv},
arxivId = {1810.09583},
author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
doi = {10.1109/MSP.2018.2867638},
eprint = {1810.09583},
issn = {15580792},
journal = {IEEE Signal Processing Magazine},
title = {{Model Selection Techniques: An Overview}},
year = {2018}
}
@misc{Johnson2004,
abstract = {Recently, researchers in several areas of ecology and evolution have begun to change the way in which they analyze data and make biological inferences. Rather than the traditional null hypothesis testing approach, they have adopted an approach called model selection, in which several competing hypotheses are simultaneously confronted with data. Model selection can be used to identify a single best model, thus lending support to one particular hypothesis, or it can be used to make inferences based on weighted support from a complete set of competing models. Model selection is widely accepted and well developed in certain fields, most notably in molecular systematics and mark-recapture analysis. However, it is now gaining support in several other areas, from molecular evolution to landscape ecology. Here, we outline the steps of model selection and highlight several ways that it is now being implemented. By adopting this approach, researchers in ecology and evolution will find a valuable alternative to traditional null hypothesis testing, especially when more than one hypothesis is plausible.},
author = {Johnson, Jerald B. and Omland, Kristian S.},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/j.tree.2003.10.013},
issn = {01695347},
title = {{Model selection in ecology and evolution}},
year = {2004}
}
@misc{Liddle2007,
abstract = {Model selection is the problem of distinguishing competing models, perhaps featuring different numbers of parameters. The statistics literature contains two distinct sets of tools, those based on information theory such as the Akaike Information Criterion (AIC), and those on Bayesian inference such as the Bayesian evidence and Bayesian Information Criterion (BIC). The Deviance Information Criterion combines ideas from both heritages; it is readily computed from Monte Carlo posterior samples and, unlike the AIC and BIC, allows for parameter degeneracy. I describe the properties of the information criteria, and as an example compute them from Wilkinson Microwave Anisotropy Probe 3-yr data for several cosmological models. I find that at present the information theory and Bayesian approaches give significantly different conclusions from that data. {\textcopyright} 2007 The Author. Journal compilation {\textcopyright} 2007 RAS.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0701113},
author = {Liddle, Andrew R.},
booktitle = {Monthly Notices of the Royal Astronomical Society: Letters},
doi = {10.1111/j.1745-3933.2007.00306.x},
eprint = {0701113},
issn = {17453933},
keywords = {Cosmology: theory,Methods: data analysis,Methods: statistical},
primaryClass = {astro-ph},
title = {{Information criteria for astrophysical model selection}},
year = {2007}
}
@misc{Dormann2018,
abstract = {In ecology, the true causal structure for a given problem is often not known, and several plausible models and thus model predictions exist. It has been claimed that using weighted averages of these models can reduce prediction error, as well as better reflect model selection uncertainty. These claims, however, are often demonstrated by isolated examples. Analysts must better understand under which conditions model averaging can improve predictions and their uncertainty estimates. Moreover, a large range of different model averaging methods exists, raising the question of how they differ in their behaviour and performance. Here, we review the mathematical foundations of model averaging along with the diversity of approaches available. We explain that the error in model-averaged predictions depends on each model's predictive bias and variance, as well as the covariance in predictions between models, and uncertainty about model weights. We show that model averaging is particularly useful if the predictive error of contributing model predictions is dominated by variance, and if the covariance between models is low. For noisy data, which predominate in ecology, these conditions will often be met. Many different methods to derive averaging weights exist, from Bayesian over information-theoretical to cross-validation optimized and resampling approaches. A general recommendation is difficult, because the performance of methods is often context dependent. Importantly, estimating weights creates some additional uncertainty. As a result, estimated model weights may not always outperform arbitrary fixed weights, such as equal weights for all models. When averaging a set of models with many inadequate models, however, estimating model weights will typically be superior to equal weights. We also investigate the quality of the confidence intervals calculated for model-averaged predictions, showing that they differ greatly in behaviour and seldom manage to achieve nominal coverage. Our overall recommendations stress the importance of non-parametric methods such as cross-validation for a reliable uncertainty quantification of model-averaged predictions.},
author = {Dormann, Carsten F. and Calabrese, Justin M. and Guillera-Arroita, Gurutzeta and Matechou, Eleni and Bahn, Volker and Barto{\'{n}}, Kamil and Beale, Colin M. and Ciuti, Simone and Elith, Jane and Gerstner, Katharina and Guelat, J{\'{e}}r{\^{o}}me and Keil, Petr and Lahoz-Monfort, Jos{\'{e}} J. and Pollock, Laura J. and Reineking, Bj{\"{o}}rn and Roberts, David R. and Schr{\"{o}}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Wood, Simon N. and W{\"{u}}est, Rafael O. and Hartig, Florian},
booktitle = {Ecological Monographs},
doi = {10.1002/ecm.1309},
issn = {15577015},
keywords = {AIC weights,ensemble,model averaging,model combination,nominal coverage,prediction averaging,uncertainty},
title = {{Model averaging in ecology: a review of Bayesian, information-theoretic, and tactical approaches for predictive inference}},
year = {2018}
}
@inproceedings{Fernandez2015,
author = {Fernandez, Miguel Angel Luque},
booktitle = {Faculty of Epidemiology and Population Health Department of Non-communicable Disease.},
title = {{Cross-validation}},
year = {2015}
}
@book{Bishop2006,
abstract = {Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field, and together they have undergone substantial development over the past ten years. In particular, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic models. Also, the practical applicability of Bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational Bayes and expectation propa- gation. Similarly, new models based on kernels have had significant impact on both algorithms and applications. This new textbook reflects these recent developments while providing a compre- hensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners, and assumes no previous knowledge of pattern recognition or ma- chine learning concepts. Knowledge of multivariate calculus and basic linear algebra is required, and some familiarity with probabilities would be helpful though not es- sential as the book includes a self-contained introduction to basic probability theory. Because this book has broad scope, it is impossible to provide a complete list of references, and in particular no attempt has been made to provide accurate historical attribution of ideas. Instead, the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what, in some cases, is a very extensive literature. For this reason, the references are often to more recent textbooks and review articles rather than to original sources.},
author = {Bishop, Christopher M.},
booktitle = {Information Science and Statistics},
isbn = {9780387310732},
title = {{Machine Learning and Pattern Recoginiton}},
year = {2006}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. ?? 1996 Kluwer Academic Publishers,.},
author = {Breiman, Leo},
doi = {10.1007/bf00058655},
issn = {08856125},
journal = {Machine Learning},
keywords = {Aggregation,Averaging,Bootstrap,Combining},
title = {{Bagging predictors}},
year = {1996}
}
@article{Bem1974,
abstract = {The historically recurring controversy over the existence of cross-situational consistencies in behavior is sustained by the discrepancy between intuitions, which affirm their existence, and the research literature, which does not. It is argued that the nomothetic assumptions of the traditional research paradigm are incorrect, and that by adopting some of the idiographic assumptions employed by intuitions, higher cross-situational correlation coefficients can be obtained. A study with 64 undergraduates is reported which shows that it is possible to identify on a priori grounds those individuals who will be cross-situationally consistent and those who will not. It is concluded that not only must personality assessment attend to situations-as has been recently urged-but to persons as well. (52 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1974 American Psychological Association.},
author = {Bem, Daryl J. and Allen, Andrea},
doi = {10.1037/h0037130},
issn = {0033295X},
journal = {Psychological Review},
keywords = {prediction of cross-situational consistent behavior, college students},
title = {{On predicting some of the people some of the time: The search for cross-situational consistencies in behavior}},
year = {1974}
}
@article{Stone1974,
abstract = {The method of cross-validatory choice and assessment is applied to prediction of a multinomial indicator. The resulting predictor is compared with analogous expressions due to Good and Fienberg {\&} Holland. Some numerical comparisons are made. {\textcopyright} 1974 Oxford University Press.},
author = {Stone, M.},
doi = {10.1093/biomet/61.3.509},
issn = {00063444},
journal = {Biometrika},
keywords = {Cross-validation,Estimation,Flattening,Multinomial,Prediction,Smoothing,Squashing},
title = {{Cross-validation and multinomial prediction}},
year = {1974}
}
@book{Lambert2018,
author = {Lambert, Ben},
title = {{A Students Guide to Bayesian Statistics}},
year = {2018}
}
@article{Yang2007,
abstract = {Theoretical developments on cross validation (CV) have mainly focused on selecting one among a list of finite-dimensional models (e.g., subset or order selection in linear regression) or selecting a smoothing parameter (e.g., bandwidth for kernel smoothing). However, little is known about consistency of cross validation when applied to compare between parametric and nonparametric methods or within nonparametric methods. We show that under some conditions, with an appropriate choice of data splitting ratio, cross validation is consistent in the sense of selecting the better procedure with probability approaching 1. Our results reveal interesting behavior of cross validation. When comparing two models (procedures) converging at the same nonparametric rate, in contrast to the parametric case, it turns out that the proportion of data used for evaluation in CV does not need to be dominating in size. Furthermore, it can even be of a smaller order than the proportion for estimation while not affecting the consistency property. {\textcopyright} Institute of Mathematical Statistics, 2007.},
author = {Yang, Yuhong},
doi = {10.1214/009053607000000514},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Consistency,Cross validation,Model selection},
title = {{Consistency of cross validation for comparing regression procedures}},
year = {2007}
}
@article{Arlot2010,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplic-ity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
archivePrefix = {arXiv},
arxivId = {0907.4728},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
eprint = {0907.4728},
issn = {19357516},
journal = {Statistics Surveys},
keywords = {Cross-validation,Leave-one-out,Model selection},
title = {{A survey of cross-validation procedures for model selection}},
year = {2010}
}
@article{Kim2009,
abstract = {We consider the accuracy estimation of a classifier constructed on a given training sample. The naive resubstitution estimate is known to have a downward bias problem. The traditional approach to tackling this bias problem is cross-validation. The bootstrap is another way to bring down the high variability of cross-validation. But a direct comparison of the two estimators, cross-validation and bootstrap, is not fair because the latter estimator requires much heavier computation. We performed an empirical study to compare the .632+ bootstrap estimator with the repeated 10-fold cross-validation and the repeated one-third holdout estimator. All the estimators were set to require about the same amount of computation. In the simulation study, the repeated 10-fold cross-validation estimator was found to have better performance than the .632+ bootstrap estimator when the classifier is highly adaptive to the training sample. We have also found that the .632+ bootstrap estimator suffers from a bias problem for large samples as well as for small samples. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Kim, Ji Hyun},
doi = {10.1016/j.csda.2009.04.009},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
year = {2009}
}
@article{Zhang2015,
abstract = {While there are various model selection methods, an unanswered but important question is how to select one of them for data at hand. The difficulty is due to that the targeted behaviors of the model selection procedures depend heavily on uncheckable or difficult-to-check assumptions on the data generating process. Fortunately, cross-validation (CV) provides a general tool to solve this problem. In this work, results are provided on how to apply CV to consistently choose the best method, yielding new insights and guidance for potentially vast amount of application. In addition, we address several seemingly widely spread misconceptions on CV.},
author = {Zhang, Yongli and Yang, Yuhong},
doi = {10.1016/j.jeconom.2015.02.006},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Adaptive procedure selection,Cross-validation,Cross-validation paradox,Data splitting ratio,Information criterion,LASSO,MCP,SCAD},
title = {{Cross-validation for selecting a model selection procedure}},
year = {2015}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximurn likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AlC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples. {\textcopyright} 1974, IEEE. All rights reserved.},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
title = {{A New Look at the Statistical Model Identification}},
year = {1974}
}
@article{Cavanaugh1997,
abstract = {The Akaike (1973, 1974) information criterion, AIC, and the corrected Akaike information criterion (Hurvich and Tsai, 1989), AICc, were both designed as estimators of the expected Kullback-Leibler discrepancy between the model generating the data and a fitted candidate model. AIC is justified in a very general framework, and as a result, offers a crude estimator of the expected discrepancy: one which exhibits a potentially high degree of negative bias in small-sample applications (Hurvich and Tsai, 1989). AICc corrects for this bias, but is less broadly applicable than AIC since its justification depends upon the form of the candidate model (Hurvich and Tsai, 1989, 1993; Hurvich et al., 1990; Bedrick and Tsai, 1994). Although AIC and AICc share the same objective, the derivations of the criteria proceed along very different lines, making it difficult to reconcile how AICc improves upon the approximations leading to AIC. To address this issue, we present a derivation which unifies the justifications of AIC and AICc in the linear regression framework.},
author = {Cavanaugh, Joseph E.},
doi = {10.1016/s0167-7152(96)00128-9},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {AIC,AICc,Information theory,Kullback-Leibler information,Model selection},
title = {{Unifying the derivations for the Akaike and corrected Akaike information criteria}},
year = {1997}
}
@article{Blankenshipa2002,
author = {Blankenshipa, Erin E. and Perkinsb, Micah W and Johnsonc, Ron J.},
doi = {10.4148/2475-7772.1200},
journal = {Conference on Applied Statistics in Agriculture},
title = {{THE INFORMATION-THEORETIC APPROACH TO MODEL SELECTION: DESCRIPTION AND CASE STUDY}},
year = {2002}
}
@article{Kullback1951,
abstract = {Volume 22, Number 1 (1951), 1-164},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
title = {{On Information and Sufficiency}},
year = {1951}
}
@misc{Park2018,
author = {Park, Barum},
booktitle = {AIC and BIC},
title = {{No Title}},
year = {2018}
}
@article{Shao1997,
abstract = {In the problem of selecting a linear model to approximate the true unknown regression model, some necessary and/or sufficient conditions are established for the asymptotic validity of various model selection procedures such as Akaike's AIC, Mallows' C p, Shibata's FPE$\lambda$, Schwarz' BIG, generalized AIC, cross-validation, and generalized cross-validation. It is found that these selection procedures can be classified into three classes according to their asymptotic behavior. Under some fairly weak conditions, the selection procedures in one class are asymptotically valid if there exist fixed-dimension correct models; the selection procedures in another class are asymptotically valid if no fixed-dimension correct model exists. The procedures in the third class are compromises of the procedures in the first two classes. Some empirical results are also presented.},
author = {Shao, Jun},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {AIC,Asymptotic loss efficiency,BIC,C p,Consistency,Cross-validation,GIC,Squared error loss},
title = {{An asymptotic theory for linear model selection}},
year = {1997}
}
@article{Liu2011,
abstract = {The Annals of Statistics, 2011, Vol.39, No.4, 2074-2102},
author = {Liu, Wei and Yang, Yuhong},
doi = {10.1214/11-aos899},
issn = {0090-5364},
journal = {The Annals of Statistics},
title = {{Parametric or nonparametric? A parametricness index for model selection}},
year = {2011}
}
@article{McQuarrie1997,
abstract = {For regression and time series model selection, Hurvich and Tsai (1989) obtained a bias correction Akaike information criterion, AICc, which provides better model order choices than the Akaike information criterion, AIC (Akaike, 1973). In this paper, we propose an alternative improved regression model selection criterion, AICu, which is an approximate unbiased estimator of Kullback-Leibler information. We show that AICu is neither a consistent (Shibata, 1986) nor an efficient (Shibata, 1980, 1981) criterion. Our simulation studies indicate that the behavior of AICu is a compromise between that of efficient (AICc) and consistent (BIC, Akaike, 1978) criteria. Specifically, AICu performs better than AICc for moderate to large sample sizes except when the true model is of infinite order. In addition, it outperforms BIC except when a true model exists and the sample size is large.},
author = {McQuarrie, Allan and Shumway, Robert and Tsai, Chih Ling},
doi = {10.1016/s0167-7152(96)00192-7},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {AIC,AICu,BIC,Kullback-Leibler information},
title = {{The model selection criterion AICu}},
year = {1997}
}
@article{Shannon1948,
author = {Shannon, C. E.},
doi = {10.1002/j.1538-7305.1948.tb01338.x},
issn = {15387305},
journal = {Bell System Technical Journal},
pmid = {9230594},
title = {{A Mathematical Theory of Communication}},
year = {1948}
}
@misc{Lambert2018a,
author = {Lambert, Ben},
title = {{Evaluating model fit through AIC, DIC, WAIC and LOO-CV}},
year = {2018}
}
@article{Gelman2014,
abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
archivePrefix = {arXiv},
arxivId = {1307.5928},
author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
doi = {10.1007/s11222-013-9416-2},
eprint = {1307.5928},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {AIC,Bayes,Cross-validation,DIC,Prediction,WAIC},
title = {{Understanding predictive information criteria for Bayesian models}},
year = {2014}
}
@misc{Gelman2004,
abstract = {Incorporating new and updated information, this second edition of THE bestselling text in Bayesian data analysis continues to emphasize practice over theory, describing how to conceptualize, perform, and critique statistical analyses from a Bayesian perspective. Its world-class authors provide guidance on all aspects of Bayesian data analysis and include examples of real statistical analyses, based on their own research, that demonstrate how to solve complicated problems. Changes in the new edition include: Stronger focus on MCMCRevision of the computational advice in Part IIINew chapters on nonlinear models and decision analysisSeveral additional applied examples from the authors' recent researchAdditional chapters on current models for Bayesian data analysis such as nonlinear models, generalized linear mixed models, and moreReorganization of chapters 6 and 7 on model checking and data collectionBayesian computation is currently at a stage where there are many reasonable ways to compute any given posterior distribution. However, the best approach is not always clear ahead of time. Reflecting this, the new edition offers a more pluralistic presentation, giving advice on performing computations from many perspectives while making clear the importance of being aware that there are different ways to implement any given iterative simulation computation. The new approach, additional examples, and updated information make Bayesian Data Analysis an excellent introductory text and a reference that working scientists will use throughout their professional life.},
archivePrefix = {arXiv},
arxivId = {1001.4656v2},
author = {Gelman, A and Carlin, J B and Stern, H S and Rubin, D B},
booktitle = {Wiley Interdisciplinary Reviews Cognitive Science},
doi = {10.1002/wcs.72},
eprint = {1001.4656v2},
isbn = {158488388X},
issn = {19395078},
title = {{Bayesian Data Analysis Second Edition.PDF}},
year = {2004}
}
@article{Kass1995,
abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: • From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. • Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. • Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. • Bayes factors are very general and do not require alternative models to be nested. • Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. • In “nonstandard” statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests. • The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. • When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. • Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. • Bayes factors are useful for guiding an evolutionary model-building process. • It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used. {\textcopyright} 1995 Taylor {\&} Francis Group, LLC.},
author = {Kass, Robert E. and Raftery, Adrian E.},
doi = {10.1080/01621459.1995.10476572},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {BIC,Bayesian hypothesis tests,Importance sampling,Laplace method,Markov chain Monte Carlo,Model selection,Monte Carlo integration,Posterior model probabilities,Posterior odds,Quadrature,Schwarz criterion,Sensitivity analysis,Strength of evidence},
title = {{Bayes factors}},
year = {1995}
}
@article{RAFTERY1999,
abstract = {... By this I mean situations in which (1) a Raftery / BAYES FACTORS 421 TABLE 3: Bayes Factors for Independence in the Belief in God/ Race Example –2log Equivalent Evidence ( Bayes  Bayes Prior Standard for an Prior Factor) Factor Deviation Association ... $\backslash$n},
author = {Raftery, ADRIAN E.},
doi = {10.1177/0049124199027003005},
issn = {0049-1241},
journal = {Sociological Methods {\&} Research},
title = {{Bayes Factors and BIC}},
year = {1999}
}
@incollection{Chipman2001,
abstract = {In principle, the Bayesian approach to model selection is straightforward. Prior probability distributions are used to describe the uncertainty surround-ing all unknowns. After observing the data, the posterior distribution pro-vides a coherent post data summary of the remaining uncertainty which is relevant for model selection. However, the practical implementation of this approach often requires carefully tailored priors and novel posterior calcula-tion methods. In this article, we illustrate some of the fundamental practical issues that arise for two different model selection problems: the variable se-lection problem for the linear model and the CART model selection problem.},
author = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E.},
doi = {10.1214/lnms/1215540964},
title = {{The Practical Implementation of Bayesian Model Selection}},
year = {2001}
}
@book{Downey2012,
abstract = {Think Bayes is an introduction to Bayesian statistics using computational methods. The premise of this book, and the other books in the Think X series, is that if you know how to program, you can use that skill to learn other topics. Most books on Bayesian statistics use mathematical notation and present ideas in terms of mathematical concepts like calculus. This book uses Python code instead of math, and discrete approximations instead of continuous mathematics. As a result, what would be an integral in a math book becomes a summation, and most operations on probability distributions are simple loops. I think this presentation is easier to understand, at least for people with programming skills. It is also more general, because when we make modeling decisions, we can choose the most appropriate model without worrying too much about whether the model lends itself to conventional analysis. Also, it provides a smooth development path from simple examples to real-world problems. Think Bayes is a Free Book. It is available under the Creative Commons Attribution-NonCommercial 3.0 Unported License, which means that you are free to copy, distribute, and modify it, as long as you attribute the work and don't use it for commercial purposes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Downey, Allen B.},
booktitle = {Green Tea Press Think X series},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4919-4542-1},
issn = {1098-6596},
pmid = {25246403},
title = {{Think Bayes: Bayesian Statistics in Python}},
year = {2012}
}
@article{VajpeyiAviSmithRory2016,
author = {{Vajpeyi Avi, Smith Rory}, Kanner Jonah},
title = {{Use of the Bayes Factor to Improve the Detection of Binary Black Hole Systems}},
year = {2016}
}
@article{Gelman1995,
abstract = {This article presents the authors' comments on the article Bayesian Model Selection in Social Research by Adrian E. Raftery in the January 1995 issue. Raftery's paper addresses two important problems in the statistical analysis of social science data: (1) choosing an appropriate model when so much data are available that standard P-values reject all parsimonious models; and (2) making estimates and predictions when there are not enough data available to fit the desired model using standard techniques. For both problems, we agree with Raftery that classical frequentist methods fail and that Raftery's suggested methods based on Bayesian information criterion (BIC) can point in better directions. Nevertheless, we disagree with his solutions because, in principle, they are still directed off-target and only by serendipity manage to hit the target in special circumstances. Our primary criticisms of Raftery's proposals are that (1) he promises the impossible: the selection of a model that is adequate for specific purposes without consideration of those purposes; and (2) he uses the same limited tool for model averaging as for model selection, thereby depriving himself of the benefits of the broad range of available Bayesian procedures.},
author = {Gelman, Andrew and Rubin, Donald B.},
doi = {10.2307/271064},
issn = {00811750},
journal = {Sociological Methodology},
title = {{Avoiding Model Selection in Bayesian Social Research}},
year = {1995}
}
@article{Vehtari2017,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
doi = {10.1007/s11222-016-9696-4},
eprint = {1507.04544},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
year = {2017}
}
@article{Watanabe2013,
abstract = {A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature 1/logn, where n is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for or unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models. {\textcopyright} 2013 Sumio Watanabe.},
archivePrefix = {arXiv},
arxivId = {1208.6338},
author = {Watanabe, Sumio},
eprint = {1208.6338},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Bayes marginal likelihood,Widely applicable Bayes information criterion},
title = {{A widely applicable bayesian information criterion}},
year = {2013}
}
@article{Burkner2019,
author = {{Paul-Christian B{\"{u}}rkner, Jonah Gabry}, Aki Vehtari},
title = {{Approximate leave-future-out cross-validation for Bayesian time series models}},
year = {2019}
}
@article{Cickovski2005,
abstract = {We present COMPUCELL3D, a software framework for three-dimensional simulation of morphogenesis in different organisms. COMPUCELL3D employs biologically relevant models for cell clustering, growth, and interaction with chemical fields. COMPUCELL3D uses design patterns for speed, efficient memory management, extensibility, and flexibility to allow an almost unlimited variety of simulations. We have verified COMPUCELL3D by building a model of growth and skeletal pattern formation in the avian (chicken) limb bud. Binaries and source code are available, along with documentation and input files for sample simulations, at http://compucell.sourceforge.net. {\textcopyright} 2005 IEEE.},
author = {Cickovski, Trevor M. and Huang, Chengbang and Chaturvedi, Rajiv and Glimm, Tilmann and Hentschel, H. George E. and Alber, Mark S. and Glazier, James A. and Newman, Stuart A. and Izaguirre, Jes{\'{u}}s A.},
doi = {10.1109/TCBB.2005.46},
issn = {15455963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
keywords = {Biological development,Cellular Potts Model (CPM),Cellular automata,Extensible Markup Language (XML),Morphogenesis,Reaction-diffusion},
title = {{A framework for three-dimensional simulation of morphogenesis}},
year = {2005}
}
@article{Wooldridge2003,
abstract = {SUMMARY: The modern approach of this book recognizes that econometrics has moved from a specialized mathematical description of economics to an applied interpretation based on empirical research techniques. It bridges the gap between the mechanics of econometrics and modern applications of econometrics by employing a systematic approach motivated by the major problems facing applied researchers today. Throughout the content, the emphasis on examples gives a concrete reality to economic relationships and allows treatment of interesting policy questions in a realistic and accessible framework.},
author = {Wooldridge, Jeffrey M},
doi = {10.1198/jasa.2006.s154},
isbn = {0324113641},
issn = {01621459},
journal = {Economic Analysis},
pmid = {1320},
title = {{Introductory Econometrics: A Modern Approach}},
year = {2003}
}
@book{Nocedal2006,
abstract = {Optimization is an important tool used in decision science and for the analysis of physical systems used in engineering. One can trace its roots to the Calculus of Variations and the work of Euler and Lagrange. This natural and reasonable approach to mathematical programming covers numerical methods for finite-dimensional optimization problems. It begins with very simple ideas progressing through more complicated concepts, concentrating on methods for both unconstrained and constrained optimization.},
author = {Nocedal, J and Wright, S},
booktitle = {Springer},
isbn = {0387987932},
title = {{Numerical optimization, series in operations research and financial engineering}},
year = {2006}
}
@misc{Cosma2015,
author = {Cosma, Shalizi},
booktitle = {https://www.stat.cmu.edu/{\~{}}cshalizi/mreg/15/lectures/21/lecture-21.pdf},
title = {{Lecture 21 : Model selection}},
year = {2015}
}
@misc{ScienceDirect2019,
author = {ScienceDirect},
booktitle = {Elsevier},
title = {{Akaike Information Criterion}},
url = {https://www.sciencedirect.com/topics/medicine-and-dentistry/akaike-information-criterion},
year = {2019}
}
@article{Schulze-hartung2014,
abstract = {Reduced chi-squared is a very popular method for model assessment, model comparison, convergence diagnostic, and error estimation in astronomy. In this manuscript, we discuss the pitfalls involved in using reduced chi-squared. There are two independent problems: (a) The number of degrees of freedom can only be estimated for linear models. Concerning nonlinear models, the number of degrees of freedom is unknown, i.e., it is not possible to compute the value of reduced chi-squared. (b) Due to random noise in the data, also the value of reduced chi-squared itself is subject to noise, i.e., the value is uncertain. This uncertainty impairs the usefulness of reduced chi-squared for differentiating between models or assessing convergence of a minimisation procedure. The impact of noise on the value of reduced chi-squared is surprisingly large, in particular for small data sets, which are very common in astrophysical problems. We conclude that reduced chi-squared can only be used with due caution for linear models, whereas it must not be used for nonlinear models at all. Finally, we recommend more sophisticated and reliable methods, which are also applicable to nonlinear models. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1012.3754v1},
author = {Schulze-hartung, Tim and Melchior, Peter},
eprint = {arXiv:1012.3754v1},
journal = {Astro-Ph},
title = {{NO Dos and don ' ts of reduced chi-squared}},
year = {2014}
}
@article{VanRavenzwaaij2018,
abstract = {Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
author = {van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D.},
doi = {10.3758/s13423-016-1015-8},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayesian inference,MCMC,Markov Chain Monte–Carlo,Tutorial},
title = {{A simple introduction to Markov Chain Monte–Carlo sampling}},
year = {2018}
}
@article{Foreman-Mackey2013,
abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman {\&} Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to {\$}\backslashsim N{\^{}}2{\$} for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
archivePrefix = {arXiv},
arxivId = {1202.3665},
author = {Foreman-Mackey, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
doi = {10.1086/670067},
eprint = {1202.3665},
issn = {00046280},
journal = {Publications of the Astronomical Society of the Pacific},
title = {{emcee : The MCMC Hammer }},
year = {2013}
}
@inproceedings{Porciani2012,
author = {Porciani, C},
booktitle = {Obervational Cosmology Lecture 3 - 2012},
title = {{Posterior Probability}},
url = {https://astro.uni-bonn.de/{~}kbasu/ObsCosmo/Slides2012/Lecture3{\_}2012.pdf},
year = {2012}
}
@misc{Evans2019,
abstract = {Evidence accumulations models (EAMs) have become the dominant modeling framework within rapid decision-making, using choice response time distributions to make inferences about the underlying decision process. These models are often applied to empirical data as “measurement tools”, with different theoretical accounts being contrasted within the framework of the model. Some method is then needed to decide between these competing theoretical accounts, as only assessing the models on their ability to fit trends in the empirical data ignores model flexibility, and therefore, creates a bias towards more flexible models. However, there is no objectively optimal method to select between models, with methods varying in both their computational tractability and theoretical basis. I provide a systematic comparison between nine different model selection methods using a popular EAM—the linear ballistic accumulator (LBA; Brown {\&} Heathcote, Cognitive Psychology 57(3), 153–178 2008)—in a large-scale simulation study and the empirical data of Dutilh et al. (Psychonomic Bulletin and Review, 1–19 2018). I find that the “predictive accuracy” class of methods (i.e., the Akaike Information Criterion [AIC], the Deviance Information Criterion [DIC], and the Widely Applicable Information Criterion [WAIC]) make different inferences to the “Bayes factor” class of methods (i.e., the Bayesian Information Criterion [BIC], and Bayes factors) in many, but not all, instances, and that the simpler methods (i.e., AIC and BIC) make inferences that are highly consistent with their more complex counterparts. These findings suggest that researchers should be able to use simpler “parameter counting” methods when applying the LBA and be confident in their inferences, but that researchers need to carefully consider and justify the general class of model selection method that they use, as different classes of methods often result in different inferences.},
author = {Evans, Nathan J.},
booktitle = {Psychonomic Bulletin and Review},
doi = {10.3758/s13423-018-01563-9},
issn = {15315320},
keywords = {Bayes factors,Decision-making,Model selection,Predictive accuracy,Response time modeling},
title = {{Assessing the practical differences between model selection methods in inferences about choice response time tasks}},
year = {2019}
}
@article{Liu2016,
abstract = {In the past 20 years, there has been a staggering increase in the use of Bayesian statistical inference, based on Markov chain Monte Carlo (MCMC) methods, to estimate model parameters and other quantities of interest. This trend exists in virtually all areas of engineering and science. In a typical application, researchers will report estimates of parametric functions (e.g., quantiles, probabilities, or predictions of future outcomes) and corresponding intervals from MCMC methods. One difficulty with the use of inferential methods based on Monte Carlo (MC) is that reported results may be inaccurate due to MC error. MC error, however, can be made arbitrarily small by increasing the number of MC draws. Most users of MCMC methods seem to use indirect diagnostics, trial-and-error, or guess-work to decide how long to run a MCMC algorithm and accuracy of MCMC output results is rarely reported. Unless careful analysis is done, reported numerical results may contain digits that are completely meaningless. In this article, we describe an algorithm to provide direct guidance on the number of MCMC draws needed to achieve a desired amount of precision (i.e., a specified number of accurate significant digits) for Bayesian credible interval endpoints.},
author = {Liu, Jia and Nordman, Daniel J. and Meeker, William Q.},
doi = {10.1080/00031305.2016.1158738},
issn = {15372731},
journal = {American Statistician},
keywords = {Monte Carlo error,Nonparametric quantile estimation,Precision},
title = {{The Number of MCMC Draws Needed to Compute Bayesian Credible Bounds}},
year = {2016}
}
@article{Patil2010,
abstract = {This user guide describes a Python package, PyMC, that allows users to efficiently code a probabilistic model and draw samples from its posterior distribution using Markov chain Monte Carlo techniques.},
author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher J.},
doi = {10.18637/jss.v035.i04},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Bayesian modeling,Markov chain monte carlo,Python,Simulation},
pmid = {21603108},
title = {{PyMC: Bayesian Stochastic modelling in Python}},
year = {2010}
}
@misc{NumPy2017,
abstract = {NumPy is the fundamental package for scientific computing with Python. It contains among other things: - a powerful N-dimensional array object - sophisticated (broadcasting) functions - tools for integrating C/C++ and Fortran code - useful linear algebra, Fourier transform, and random number capabilities Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. NumPy is licensed under the BSD license, enabling reuse with few restrictions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {NumPy},
booktitle = {NumPy Website},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{NumPy — NumPy}},
year = {2017}
}
@incollection{Hill2016,
abstract = {SciPy (pronounced “Sigh Pie”) is an open source Python library used by scientists, analysts, and engineers doing scientific computing and technical computing. SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering. SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy. There is an expanding set of scientific computing libraries that are being added to the NumPy stack every day. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack. SciPy is also a family of conferences for users and developers of these tools: SciPy (in the United States), EuroSciPy (in Europe) and SciPy.in (in India). Enthought originated the SciPy conference in the United States and continues to sponsor many of the international conferences as well as host the SciPy website. The SciPy library is currently distributed under the BSD license, and its development is sponsored and supported by an open community of developers. It is also supported by Numfocus which is a community foundation for supporting reproducible and accessible science.},
author = {Hill, Christian and Hill, Christian},
booktitle = {Learning Scientific Programming with Python},
doi = {10.1017/cbo9781139871754.008},
title = {{SciPy}},
year = {2016}
}
@article{Firat2016,
author = {Firat, Mehmet and Karaman, Emre and Başer, Ebru and Narinc, Dogan},
year = {2016},
month = {09},
pages = {19-26},
title = {Bayesian Analysis for the Comparison of Nonlinear Regression Model Parameters: an Application to the Growth of Japanese Quail},
volume = {18},
journal = {Revista Brasileira de Ciência Avícola},
doi = {10.1590/1806-9061-2015-0066}
}
@article{Gerrodette1987,
abstract = {A power analysis allows estimation of the probability of detecting upward or downward trends in abundance using linear regression, given number of samples and estimates of sample variability and rate of change. Alternatively, the minimum number or precision of samples required to detect trends with a given degree of confidence can be computed. Results are summarized graphically and, as an example, applied to the monitoring of the California sea otter Enhydra lutris population with aerial surveys.-from Author},
author = {Gerrodette, T.},
doi = {10.2307/1939220},
issn = {00129658},
journal = {Ecology},
title = {{A power analysis for detecting trends.}},
year = {1987}
}
@misc{Aho2014,
author = {Aho, Ken and Derryberry, Dewayne and Peterson, Teri},
booktitle = {Ecology},
doi = {10.1890/13-1452.1},
issn = {00129658},
pmid = {24804445},
title = {{Model selection for ecologists: The worldviews of AIC and BIC}},
year = {2014}
}
@misc{Strauss1991,
abstract = {The diversity of indirect interactions that can occur within communities is large. Recent research on indirect interactions is scattered in the literature under numerous labels. The definition of indirect effects is an important aspect of their study, and clarifies some of the subtle differences among indirect effects found in natural communities. Choosing which species to study, how to manipulate species and for what duration, which attributes to measure and, finally, which analytical techniques to use are all problems facing the community ecologist. Ultimately, we are striving for the best means of determining the relative importance of direct and indirect effects in structuring communities. {\textcopyright} 1991.},
author = {Strauss, Sharon Y.},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/0169-5347(91)90023-Q},
issn = {01695347},
title = {{Indirect effects in community ecology: Their definition, study and importance}},
year = {1991}
}
